# 音频分类Pipeline: Hum/口哨歌曲识别
数据集路径：/Users/panmingh/Code/ML_Coursework/Data/MLEndHWII_sample_800
运行之前启动conda环境panmingh
## 1. 问题定义

| 项目 | 内容 |
|------|------|
| **任务** | 8分类音频识别 (8首歌曲的hum/口哨版本) |
| **数据** | 800条音频样本，每类约100条 |
| **核心挑战** | 小样本 + 人声音高不准确 |
| **目标** | 构建可部署的分类系统，在独立测试集上评估泛化性能 |

---

## 2. 核心设计理念

### 2.1 关于特征选择

哼唱/口哨的本质是**旋律信息**，而人哼唱时音高往往不准。因此：

- ❌ **绝对音高(F0)不可靠** — 同一首歌，不同人哼唱的音高可能差几个八度
- ✅ **相对音高变化(音程)稳定** — 旋律的"走向"是不变的
- ✅ **节奏模式稳定** — 人会哼错音，但节拍通常正确

**特征设计原则**: 提取**音高归一化特征**和**相对变化特征**，而非绝对值。

### 2.2 关于训练与评估

遵循**训练-部署-测试**范式：

```
┌─────────────────────────────────────────────────────────────────┐
│  Phase 1: 开发阶段 (仅使用 Train + Val)                          │
│  ├── 特征工程、模型选择、超参调优                                 │
│  └── 所有决策基于Val集性能，禁止触碰Test集                        │
├─────────────────────────────────────────────────────────────────┤
│  Phase 2: 部署阶段                                               │
│  ├── 锁定最终模型和超参                                          │
│  └── 在 Train+Val 上重新训练最终模型                             │
├─────────────────────────────────────────────────────────────────┤
│  Phase 3: 测试阶段 (仅一次)                                      │
│  └── 在Test集上评估，报告最终性能                                │
└─────────────────────────────────────────────────────────────────┘
```

### 2.3 关于评估指标

选择**Macro-AUC**作为主指标：

| 指标 | 选择理由 |
|------|----------|
| **Macro-AUC** (主指标) | 衡量模型区分各类别的能力，对类别不平衡鲁棒，综合考虑排序质量 |
| Accuracy (辅助) | 直观，便于理解 |
| Macro-F1 (辅助) | 综合Precision和Recall |

---

## 3. Pipeline总览

```
数据 ──→ 特征提取 ──→ 特征可视化 ──→ 无监督分析 ──→ 特征筛选 ──→ 模型训练 ──→ 部署 ──→ 测试
              │              │              │              │              │
              ↓              ↓              ↓              ↓              ↓
        数据增强(仅Train)  分布/降维图    聚类验证     RF重要性排序    5-Fold CV
```

---

## 4. 数据划分

```python
# 分层采样，保证各类别比例一致
Train : Val : Test = 70% : 15% : 15%
# 即: 560 : 120 : 120 条
```

**关键原则**:
- Test集在最终评估前**完全隔离**
- 数据增强**仅作用于Train集**
- 特征统计量(均值/方差)**仅从Train集计算**，应用于Val/Test

---

## 5. 数据增强 (仅Train集)

针对小样本问题和音高不准特性，采用以下增强策略：

| 增强方法 | 参数范围 | 目的 |
|----------|----------|------|
| Pitch Shift | ±3 semitones | **核心增强**: 模拟人哼唱音高偏差 |
| Time Stretch | 0.9x ~ 1.1x | 模拟唱快/唱慢 |
| 添加噪声 | SNR 20~40dB | 提高噪声鲁棒性 |

**增强策略**: 每个Train样本增强2~3倍，最终Train集约1500~2000条。

---

## 6. 特征工程

### 6.1 特征体系

我们提取三类互补特征：

| 特征类别 | 具体内容 | 维度 | 捕捉信息 |
|----------|----------|------|----------|
| **频谱特征** | MFCC(13维) × 统计量(mean,std,Δmean,Δstd) | 52 | 音色、频谱包络 |
| **韵律特征** | 归一化F0 + 音程序列统计 | 20 | 旋律走向 (核心) |
| **节奏特征** | Onset间隔统计 + Tempo | 10 | 节拍模式 |
| **总计** | | **~82维** | |

### 6.2 特征详细说明

**频谱特征 (MFCC)**
```
- 13个MFCC系数，每帧提取
- 对整段音频计算: mean, std (26维)
- 对一阶差分计算: mean, std (26维)
- 共52维
```

**韵律特征 (F0相关)** — 核心特征
```
- 使用pyin算法提取F0轨迹
- 归一化F0: (F0 - mean) / std → 统计量 (5维)
- 音程序列: 相邻帧的音高差(semitones) → 统计量 (10维)
- 旋律轮廓: 上升/下降/平稳帧占比 (3维)
- 浊音比例: voiced_frames / total_frames (1维)
- 共约20维
```

**节奏特征**
```
- Onset检测 → 计算IOI(Inter-Onset-Interval)
- IOI统计: mean, std, max, min (4维)
- Tempo估计 (1维)
- Onset密度: onsets_per_second (1维)
- 共约10维
```

---

## 7. 特征可视化分析

在进入建模之前，通过可视化深入理解特征空间的结构。

### 7.1 单特征分布可视化

| 可视化类型 | 内容 | 目的 |
|------------|------|------|
| **箱线图 (Box Plot)** | 各类别在核心特征上的分布 | 观察类间是否有明显差异 |
| **小提琴图 (Violin Plot)** | 特征分布的形状对比 | 观察分布是否有区分度 |
| **核密度图 (KDE)** | 各类别特征的概率密度 | 观察重叠程度 |

**重点关注**: 音程特征(interval_mean, interval_std)和节奏特征(tempo, IOI_std)在不同歌曲间的差异。

### 7.2 特征相关性分析

- **相关性热力图 (Correlation Heatmap)**: 识别高度相关的特征对 (|r| > 0.9)
- **目的**: 发现冗余特征，为后续特征筛选提供依据

### 7.3 降维可视化

| 方法 | 特点 | 观察目的 |
|------|------|----------|
| **PCA (2D/3D)** | 线性降维，保留方差 | 观察类别的线性可分性 |
| **t-SNE** | 非线性，保留局部结构 | 观察类别是否形成簇 |

**关键问题**: 8个类别在特征空间中是否自然分离？哪些类别靠得近（可能难以区分）？

---

## 8. 无监督学习分析

**核心思想**: 在使用标签训练模型之前，先验证特征本身是否具有区分不同歌曲的能力。

### 8.1 聚类分析

使用**K-Means (K=8)** 对特征进行聚类，然后与真实标签对比：

| 评估指标 | 含义 | 期望 |
|----------|------|------|
| **NMI (归一化互信息)** | 聚类结果与真实标签的一致性 | > 0.5 说明特征有效 |
| **ARI (调整兰德指数)** | 校正随机性的聚类准确度 | > 0.3 说明特征有效 |
| **Silhouette Score** | 聚类的紧凑度和分离度 | > 0.2 说明结构清晰 |

### 8.2 聚类结果分析

- **聚类-真实标签混淆矩阵**: 观察哪些歌曲被聚到同一簇
- **意义**: 如果两首歌在无监督下就容易混淆，有监督时也可能难以区分

### 8.3 无监督分析的价值

| 如果NMI较高 (>0.5) | 如果NMI较低 (<0.3) |
|-------------------|-------------------|
| 特征设计有效，可以进入建模 | 需要回顾特征工程，可能缺少关键特征 |
| 简单模型可能就够用 | 可能需要更复杂的模型或更多特征 |

---

## 9. 特征筛选

基于前面的可视化和无监督分析结果，进行有针对性的特征筛选。

### 9.1 筛选方法

使用**Random Forest特征重要性**进行筛选：

1. 在Train集上训练RF分类器
2. 提取`feature_importances_`
3. 可视化Top-20重要特征柱状图
4. 选择Top-K重要特征 (K通过Val集确定，预计30~50维)

### 9.2 筛选依据整合

| 来源 | 筛选动作 |
|------|----------|
| 相关性热力图 | 去除高度相关特征对中的一个 (\|r\| > 0.95) |
| RF重要性 | 保留importance排名前K的特征 |
| 无监督分析 | 若某类特征对聚类贡献大，优先保留 |

**筛选目的**: 去除冗余特征，降低过拟合风险，提升泛化能力。

---

## 10. 模型选择

考虑到数据规模(800条)和特征维度(~50维)，我们设计两类模型进行对比：

### 10.1 传统机器学习方法

基于筛选后的手工特征向量进行分类。

| 模型 | 选择理由 | 超参搜索空间 |
|------|----------|--------------|
| **KNN** | 最直观的分类器，"看邻居投票"，易于理解 | K∈{3,5,7,9}, weights∈{uniform, distance} |
| **Random Forest** | 可解释性好，不易过拟合 | n_estimators∈{100,200}, max_depth∈{5,10,None} |

### 10.2 深度学习方法

端到端学习，直接从频谱特征到分类。

| 模型 | 输入 | 结构设计 |
|------|------|----------|
| **MLP** | 手工特征向量 (~50维) | FC(128)→ReLU→Dropout→FC(64)→ReLU→FC(8) |
| **1D-CNN** | MFCC序列 (T×13) | Conv1D→BN→ReLU→Pool→Conv1D→GlobalPool→FC(8) |

**深度学习注意事项**:
- 数据量较小，需使用Dropout和早停防止过拟合
- 学习率: 1e-3 ~ 1e-4，使用Adam优化器
- 配合数据增强缓解过拟合

### 10.3 模型对比设计

| 对比维度 | 传统ML (KNN/RF) | 深度学习 (MLP/CNN) |
|----------|-----------------|-------------------|
| 输入 | 手工特征 | 手工特征 / 原始序列 |
| 可解释性 | 高 | 低 |
| 数据需求 | 小 | 大 |
| 调参难度 | 低 | 高 |

**模型选择流程**:
1. 5-Fold Stratified CV 在Train集上评估
2. 以Val集Macro-AUC为标准选择最优模型和超参
3. 最终选定一个模型进入部署阶段

---

## 11. 训练与验证流程

### 11.1 开发阶段

```
输入: Train集 (560条, 增强后~1500条), Val集 (120条)

For each 模型类型 in [KNN, RF, MLP, CNN]:
    For each 超参组合 in 搜索空间:
        scores = 5-Fold-CV(Train集, 超参)
        记录 mean_cv_score
    
    选择最优超参 → 在完整Train集训练 → 在Val集评估
    记录 Val集 Macro-AUC

选择 Val集 Macro-AUC 最高的模型作为最终模型
```

### 11.2 部署阶段

```
输入: Train集 + Val集 (共680条, 增强后~2000条)
模型: 开发阶段确定的最优模型和超参

在 Train+Val 上训练最终模型
保存模型 → model.pkl
```

### 11.3 测试阶段 (仅执行一次)

```
输入: Test集 (120条, 无增强)
模型: model.pkl

预测 → 计算指标 → 报告最终性能
```

---

## 12. 评估与分析

### 12.1 报告指标

| 指标 | 说明 |
|------|------|
| **Macro-AUC** | 主指标，各类别AUC的平均 |
| Accuracy | 整体准确率 |
| Macro-F1 | 各类别F1的平均 |
| 混淆矩阵 | 分析类间混淆情况 |

### 12.2 消融实验

在Val集上进行，验证设计决策的有效性：

| 实验 | 目的 |
|------|------|
| 仅MFCC vs 仅韵律 vs 全特征 | 验证各类特征的贡献 |
| 有/无数据增强 | 验证增强策略的有效性 |
| 有/无特征筛选 | 验证特征筛选的必要性 |

### 12.3 可视化

- 特征重要性柱状图 (Top-20)
- 混淆矩阵热力图
- 各类别ROC曲线

---

## 13. 文件结构

```
project/
├── data/
│   ├── raw/                # 原始音频
│   └── processed/          # 预处理后
├── src/
│   ├── features.py         # 特征提取模块
│   ├── augment.py          # 数据增强模块
│   ├── visualize.py        # 可视化模块
│   ├── unsupervised.py     # 无监督分析模块
│   ├── train.py            # 训练流程
│   └── evaluate.py         # 评估模块
├── models/
│   └── final_model.pkl     # 部署模型
├── results/
│   ├── figures/            # 可视化图表
│   └── metrics.json        # 评估指标
└── main.py                 # 主入口
```

---

## 14. 总结

| 设计要点 | 我们的选择 | 理由 |
|----------|------------|------|
| 核心特征 | 归一化F0 + 音程序列 | 应对"音高不准"问题 |
| 数据增强 | Pitch Shift为主 | 直接模拟音高偏差 |
| 特征可视化 | 箱线图 + 降维图 | 理解特征空间结构 |
| 无监督分析 | K-Means聚类验证 | 验证特征有效性，指导建模 |
| 特征筛选 | RF重要性 | 简单有效，可解释 |
| 模型 | 传统ML(KNN/RF) + 深度学习(MLP/CNN) | 两类方法对比，易理解且适合小样本 |
| 主指标 | Macro-AUC | 综合、鲁棒 |
| 评估范式 | Train→Val→锁定→Test | 保证泛化性评估的公正性 |