{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBU5201 Mini-Project\n",
    "# 1. Author\n",
    "\n",
    "**Student Name:** Minghui Pan\n",
    "\n",
    "**Student ID:** 231220208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0) Environment setup (keeps CPU threading stable inside notebooks)\n",
    "# ---------------------------------------------------------------------\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMBA_DISABLE_JIT\", \"1\")\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths and constants (edit for your machine)\n",
    "# ---------------------------------------------------------------------\n",
    "DATASET_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/Data/MLEndHWII_sample_800\")\n",
    "DEFAULT_OUTPUT_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results\")\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:  # pragma: no cover - fallback when tqdm is unavailable\n",
    "    tqdm = None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Configuration object\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    sr: int = 22050\n",
    "    n_mfcc: int = 13\n",
    "    hop_length: int = 512\n",
    "    fmin: float = librosa.note_to_hz(\"C2\")\n",
    "    fmax: float = librosa.note_to_hz(\"C7\")\n",
    "    onset_backtrack: bool = True\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Small numeric helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def _safe_stats(x: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "    if x.size == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "    return float(np.mean(x)), float(np.std(x)), float(np.max(x)), float(np.min(x))\n",
    "\n",
    "\n",
    "def _safe_mean_std(x: np.ndarray) -> Tuple[float, float]:\n",
    "    if x.size == 0:\n",
    "        return 0.0, 0.0\n",
    "    return float(np.mean(x)), float(np.std(x))\n",
    "\n",
    "\n",
    "def _nan_to_num(x: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Audio loading\n",
    "# ---------------------------------------------------------------------\n",
    "def load_audio(path: str, cfg: FeatureConfig) -> Tuple[np.ndarray, int]:\n",
    "    y, sr = librosa.load(path, sr=cfg.sr, mono=True)\n",
    "    if y.size == 0:\n",
    "        return np.zeros(1, dtype=np.float32), cfg.sr\n",
    "    return y, sr\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) MFCC + delta features\n",
    "# ---------------------------------------------------------------------\n",
    "def mfcc_features(y: np.ndarray, cfg: FeatureConfig) -> List[float]:\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y, sr=cfg.sr, n_mfcc=cfg.n_mfcc, hop_length=cfg.hop_length\n",
    "    )\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "    mfcc = _nan_to_num(mfcc)\n",
    "    delta = _nan_to_num(delta)\n",
    "\n",
    "    feats: List[float] = []\n",
    "    feats.extend(np.mean(mfcc, axis=1).tolist())\n",
    "    feats.extend(np.std(mfcc, axis=1).tolist())\n",
    "    feats.extend(np.mean(delta, axis=1).tolist())\n",
    "    feats.extend(np.std(delta, axis=1).tolist())\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) F0 / pitch contour features (pyin)\n",
    "# ---------------------------------------------------------------------\n",
    "def f0_features(y: np.ndarray, cfg: FeatureConfig) -> List[float]:\n",
    "    f0, voiced_flag, voiced_prob = librosa.pyin(\n",
    "        y,\n",
    "        fmin=cfg.fmin,\n",
    "        fmax=cfg.fmax,\n",
    "        sr=cfg.sr,\n",
    "        hop_length=cfg.hop_length,\n",
    "    )\n",
    "    f0 = _nan_to_num(f0)\n",
    "    voiced_flag = voiced_flag.astype(np.float32) if voiced_flag is not None else None\n",
    "\n",
    "    voiced_ratio = 0.0\n",
    "    if voiced_flag is not None and voiced_flag.size > 0:\n",
    "        voiced_ratio = float(np.mean(voiced_flag))\n",
    "\n",
    "    f0_mean, f0_std = _safe_mean_std(f0)\n",
    "    if f0_std > 0:\n",
    "        f0_norm = (f0 - f0_mean) / f0_std\n",
    "    else:\n",
    "        f0_norm = f0 - f0_mean\n",
    "\n",
    "    f0n_mean, f0n_std = _safe_mean_std(f0_norm)\n",
    "    f0n_max, f0n_min = (float(np.max(f0_norm)), float(np.min(f0_norm))) if f0_norm.size else (0.0, 0.0)\n",
    "    f0n_median = float(np.median(f0_norm)) if f0_norm.size else 0.0\n",
    "\n",
    "    # Interval in semitones between adjacent frames.\n",
    "    f0_nonzero = np.where(f0 > 0, f0, np.nan)\n",
    "    intervals = 12.0 * np.log2(f0_nonzero[1:] / f0_nonzero[:-1])\n",
    "    intervals = _nan_to_num(intervals)\n",
    "\n",
    "    int_mean, int_std, int_max, int_min = _safe_stats(intervals)\n",
    "    int_median = float(np.median(intervals)) if intervals.size else 0.0\n",
    "    int_iqr = float(np.percentile(intervals, 75) - np.percentile(intervals, 25)) if intervals.size else 0.0\n",
    "    int_abs_mean = float(np.mean(np.abs(intervals))) if intervals.size else 0.0\n",
    "    int_abs_std = float(np.std(np.abs(intervals))) if intervals.size else 0.0\n",
    "    int_pos_ratio = float(np.mean(intervals > 0)) if intervals.size else 0.0\n",
    "    int_neg_ratio = float(np.mean(intervals < 0)) if intervals.size else 0.0\n",
    "\n",
    "    # Melodic contour proportions.\n",
    "    eps = 1e-4\n",
    "    up_ratio = float(np.mean(intervals > eps)) if intervals.size else 0.0\n",
    "    down_ratio = float(np.mean(intervals < -eps)) if intervals.size else 0.0\n",
    "    flat_ratio = float(np.mean(np.abs(intervals) <= eps)) if intervals.size else 0.0\n",
    "\n",
    "    feats = [\n",
    "        f0n_mean,\n",
    "        f0n_std,\n",
    "        f0n_max,\n",
    "        f0n_min,\n",
    "        f0n_median,\n",
    "        int_mean,\n",
    "        int_std,\n",
    "        int_max,\n",
    "        int_min,\n",
    "        int_median,\n",
    "        int_iqr,\n",
    "        int_abs_mean,\n",
    "        int_abs_std,\n",
    "        int_pos_ratio,\n",
    "        int_neg_ratio,\n",
    "        up_ratio,\n",
    "        down_ratio,\n",
    "        flat_ratio,\n",
    "        voiced_ratio,\n",
    "        float(np.mean(_nan_to_num(voiced_prob))) if voiced_prob is not None else 0.0,\n",
    "    ]\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) Rhythm / onset features\n",
    "# ---------------------------------------------------------------------\n",
    "def _estimate_tempo(onset_env: np.ndarray, cfg: FeatureConfig) -> float:\n",
    "    if onset_env.size < 2:\n",
    "        return 0.0\n",
    "    onset_env = onset_env - np.mean(onset_env)\n",
    "    if np.allclose(onset_env, 0.0):\n",
    "        return 0.0\n",
    "    ac = np.correlate(onset_env, onset_env, mode=\"full\")[onset_env.size - 1 :]\n",
    "    min_bpm, max_bpm = 30.0, 240.0\n",
    "    min_lag = int((60.0 * cfg.sr) / (max_bpm * cfg.hop_length))\n",
    "    max_lag = int((60.0 * cfg.sr) / (min_bpm * cfg.hop_length))\n",
    "    min_lag = max(min_lag, 1)\n",
    "    max_lag = min(max_lag, ac.size - 1)\n",
    "    if max_lag <= min_lag:\n",
    "        return 0.0\n",
    "    lag = int(np.argmax(ac[min_lag : max_lag + 1]) + min_lag)\n",
    "    return float(60.0 * cfg.sr / (cfg.hop_length * lag))\n",
    "\n",
    "\n",
    "def rhythm_features(y: np.ndarray, cfg: FeatureConfig) -> List[float]:\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=cfg.sr, hop_length=cfg.hop_length)\n",
    "    onset_frames = librosa.onset.onset_detect(\n",
    "        onset_envelope=onset_env,\n",
    "        sr=cfg.sr,\n",
    "        hop_length=cfg.hop_length,\n",
    "        backtrack=cfg.onset_backtrack,\n",
    "    )\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=cfg.sr, hop_length=cfg.hop_length)\n",
    "    ioi = np.diff(onset_times)\n",
    "    ioi_mean, ioi_std, ioi_max, ioi_min = _safe_stats(ioi)\n",
    "\n",
    "    tempo = _estimate_tempo(onset_env, cfg)\n",
    "    duration = float(len(y)) / float(cfg.sr) if cfg.sr > 0 else 0.0\n",
    "    onsets_per_sec = float(len(onset_times) / duration) if duration > 0 else 0.0\n",
    "\n",
    "    return [\n",
    "        ioi_mean,\n",
    "        ioi_std,\n",
    "        ioi_max,\n",
    "        ioi_min,\n",
    "        tempo,\n",
    "        onsets_per_sec,\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8) Full feature vector helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_features(path: str, cfg: FeatureConfig | None = None) -> np.ndarray:\n",
    "    cfg = cfg or FeatureConfig()\n",
    "    y, _ = load_audio(path, cfg)\n",
    "    feats: List[float] = []\n",
    "    feats.extend(mfcc_features(y, cfg))\n",
    "    feats.extend(f0_features(y, cfg))\n",
    "    feats.extend(rhythm_features(y, cfg))\n",
    "    return np.asarray(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "def batch_extract(\n",
    "    paths: Iterable[str], cfg: FeatureConfig | None = None\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    cfg = cfg or FeatureConfig()\n",
    "    features: List[np.ndarray] = []\n",
    "    ok_paths: List[str] = []\n",
    "    path_list = list(paths)\n",
    "    if tqdm is not None:\n",
    "        iterator = tqdm(path_list, desc=\"Extracting features\", unit=\"file\")\n",
    "    else:\n",
    "        iterator = path_list\n",
    "    for idx, p in enumerate(iterator, start=1):\n",
    "        feats = extract_features(p, cfg)\n",
    "        features.append(feats)\n",
    "        ok_paths.append(p)\n",
    "        if tqdm is None and idx % 50 == 0:\n",
    "            print(f\"Processed {idx}/{len(path_list)} files...\")\n",
    "    return np.vstack(features), ok_paths\n",
    "\n",
    "\n",
    "def feature_names(cfg: FeatureConfig | None = None) -> List[str]:\n",
    "    cfg = cfg or FeatureConfig()\n",
    "    names: List[str] = []\n",
    "    for i in range(cfg.n_mfcc):\n",
    "        names.append(f\"mfcc_mean_{i+1}\")\n",
    "    for i in range(cfg.n_mfcc):\n",
    "        names.append(f\"mfcc_std_{i+1}\")\n",
    "    for i in range(cfg.n_mfcc):\n",
    "        names.append(f\"mfcc_delta_mean_{i+1}\")\n",
    "    for i in range(cfg.n_mfcc):\n",
    "        names.append(f\"mfcc_delta_std_{i+1}\")\n",
    "\n",
    "    names.extend(\n",
    "        [\n",
    "            \"f0n_mean\",\n",
    "            \"f0n_std\",\n",
    "            \"f0n_max\",\n",
    "            \"f0n_min\",\n",
    "            \"f0n_median\",\n",
    "            \"interval_mean\",\n",
    "            \"interval_std\",\n",
    "            \"interval_max\",\n",
    "            \"interval_min\",\n",
    "            \"interval_median\",\n",
    "            \"interval_iqr\",\n",
    "            \"interval_abs_mean\",\n",
    "            \"interval_abs_std\",\n",
    "            \"interval_pos_ratio\",\n",
    "            \"interval_neg_ratio\",\n",
    "            \"contour_up_ratio\",\n",
    "            \"contour_down_ratio\",\n",
    "            \"contour_flat_ratio\",\n",
    "            \"voiced_ratio\",\n",
    "            \"voiced_prob_mean\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    names.extend(\n",
    "        [\n",
    "            \"ioi_mean\",\n",
    "            \"ioi_std\",\n",
    "            \"ioi_max\",\n",
    "            \"ioi_min\",\n",
    "            \"tempo\",\n",
    "            \"onsets_per_sec\",\n",
    "        ]\n",
    "    )\n",
    "    return names\n",
    "\n",
    "\n",
    "def as_dict(path: str, cfg: FeatureConfig | None = None) -> Dict[str, float]:\n",
    "    cfg = cfg or FeatureConfig()\n",
    "    feats = extract_features(path, cfg)\n",
    "    names = feature_names(cfg)\n",
    "    return {k: float(v) for k, v in zip(names, feats)}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 9) Dataset helpers (metadata + saving)\n",
    "# ---------------------------------------------------------------------\n",
    "def _parse_metadata(path: Path) -> Dict[str, str]:\n",
    "    stem = path.stem\n",
    "    parts = stem.split(\"_\")\n",
    "    meta = {\"subject\": \"\", \"mode\": \"\", \"take\": \"\", \"song\": \"\"}\n",
    "    if len(parts) >= 4:\n",
    "        meta[\"subject\"] = parts[0]\n",
    "        meta[\"mode\"] = parts[1]\n",
    "        meta[\"take\"] = parts[2]\n",
    "        meta[\"song\"] = \"_\".join(parts[3:])\n",
    "    else:\n",
    "        meta[\"song\"] = stem\n",
    "    return meta\n",
    "\n",
    "\n",
    "def collect_wav_paths(data_dir: Path) -> List[Path]:\n",
    "    return sorted([p for p in data_dir.glob(\"*.wav\") if p.is_file()])\n",
    "\n",
    "\n",
    "def save_features(\n",
    "    features: np.ndarray,\n",
    "    paths: List[str],\n",
    "    names: List[str],\n",
    "    meta: List[Dict[str, str]],\n",
    "    out_dir: Path,\n",
    "    prefix: str,\n",
    ") -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    npz_path = out_dir / f\"{prefix}.npz\"\n",
    "    csv_path = out_dir / f\"{prefix}.csv\"\n",
    "\n",
    "    labels = [m.get(\"song\", \"\") for m in meta]\n",
    "    np.savez_compressed(\n",
    "        npz_path,\n",
    "        X=features,\n",
    "        labels=np.asarray(labels),\n",
    "        paths=np.asarray(paths),\n",
    "        feature_names=np.asarray(names),\n",
    "        subject=np.asarray([m.get(\"subject\", \"\") for m in meta]),\n",
    "        mode=np.asarray([m.get(\"mode\", \"\") for m in meta]),\n",
    "        take=np.asarray([m.get(\"take\", \"\") for m in meta]),\n",
    "    )\n",
    "\n",
    "    header = [\"path\", \"label\", \"subject\", \"mode\", \"take\"] + names\n",
    "    with csv_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for row_idx, p in enumerate(paths):\n",
    "            row = [\n",
    "                p,\n",
    "                labels[row_idx],\n",
    "                meta[row_idx].get(\"subject\", \"\"),\n",
    "                meta[row_idx].get(\"mode\", \"\"),\n",
    "                meta[row_idx].get(\"take\", \"\"),\n",
    "            ]\n",
    "            feat_str = [f\"{v:.8f}\" for v in features[row_idx].tolist()]\n",
    "            f.write(\",\".join(row + feat_str) + \"\\n\")\n",
    "# NOTE: We already extracted and saved the features offline, so re-running\n",
    "# this block is unnecessary and can be time-consuming.\n",
    "# cfg = FeatureConfig()\n",
    "# wav_paths = collect_wav_paths(DATASET_DIR)\n",
    "# features, ok_paths = batch_extract([str(p) for p in wav_paths], cfg)\n",
    "# meta = [_parse_metadata(Path(p)) for p in ok_paths]\n",
    "# names = feature_names(cfg)\n",
    "# save_features(features, ok_paths, names, meta, DEFAULT_OUTPUT_DIR, \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##数据增强代码\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths (edit for your machine)\n",
    "# ---------------------------------------------------------------------\n",
    "DATASET_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/Data/MLEndHWII_sample_800\")\n",
    "DEFAULT_OUTPUT_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/data/augmented\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Augmentation config\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class AugmentConfig:\n",
    "    sr: int = 22050\n",
    "    pitch_shift_steps: Tuple[int, int] = (-3, 3)\n",
    "    time_stretch_range: Tuple[float, float] = (0.9, 1.1)\n",
    "    snr_db_range: Tuple[float, float] = (20.0, 40.0)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Audio loading\n",
    "# ---------------------------------------------------------------------\n",
    "def load_audio(path: str | Path, cfg: AugmentConfig) -> np.ndarray:\n",
    "    y, _ = librosa.load(str(path), sr=cfg.sr, mono=True)\n",
    "    if y.size == 0:\n",
    "        return np.zeros(1, dtype=np.float32)\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Augmentation ops\n",
    "# ---------------------------------------------------------------------\n",
    "def pitch_shift(y: np.ndarray, cfg: AugmentConfig, rng: np.random.Generator) -> np.ndarray:\n",
    "    steps = rng.integers(cfg.pitch_shift_steps[0], cfg.pitch_shift_steps[1] + 1)\n",
    "    if steps == 0:\n",
    "        return y\n",
    "    return librosa.effects.pitch_shift(y, sr=cfg.sr, n_steps=int(steps))\n",
    "\n",
    "\n",
    "def time_stretch(y: np.ndarray, cfg: AugmentConfig, rng: np.random.Generator) -> np.ndarray:\n",
    "    rate = rng.uniform(cfg.time_stretch_range[0], cfg.time_stretch_range[1])\n",
    "    if np.isclose(rate, 1.0):\n",
    "        return y\n",
    "    return librosa.effects.time_stretch(y, rate=float(rate))\n",
    "\n",
    "\n",
    "def add_noise(y: np.ndarray, cfg: AugmentConfig, rng: np.random.Generator) -> np.ndarray:\n",
    "    snr_db = rng.uniform(cfg.snr_db_range[0], cfg.snr_db_range[1])\n",
    "    if y.size == 0:\n",
    "        return y\n",
    "    signal_power = np.mean(y**2) + 1e-12\n",
    "    snr_linear = 10 ** (snr_db / 10.0)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    noise = rng.normal(0.0, np.sqrt(noise_power), size=y.shape).astype(np.float32)\n",
    "    return y + noise\n",
    "\n",
    "\n",
    "def augment_sample(\n",
    "    y: np.ndarray, cfg: AugmentConfig, rng: np.random.Generator\n",
    ") -> List[np.ndarray]:\n",
    "    variants = []\n",
    "    variants.append(pitch_shift(y, cfg, rng))\n",
    "    variants.append(time_stretch(y, cfg, rng))\n",
    "    variants.append(add_noise(y, cfg, rng))\n",
    "    return variants\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Batch helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def augment_path(\n",
    "    path: str | Path,\n",
    "    cfg: AugmentConfig | None = None,\n",
    "    seed: int | None = None,\n",
    ") -> List[np.ndarray]:\n",
    "    cfg = cfg or AugmentConfig()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = load_audio(path, cfg)\n",
    "    return augment_sample(y, cfg, rng)\n",
    "\n",
    "\n",
    "def batch_augment(\n",
    "    paths: Iterable[str | Path],\n",
    "    cfg: AugmentConfig | None = None,\n",
    "    seed: int | None = None,\n",
    ") -> List[Tuple[Path, List[np.ndarray]]]:\n",
    "    cfg = cfg or AugmentConfig()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    out: List[Tuple[Path, List[np.ndarray]]] = []\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        y = load_audio(path, cfg)\n",
    "        out.append((path, augment_sample(y, cfg, rng)))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Saving to .wav (optional)\n",
    "# ---------------------------------------------------------------------\n",
    "def _normalize_to_int16(y: np.ndarray) -> np.ndarray:\n",
    "    if y.size == 0:\n",
    "        return np.zeros(1, dtype=np.int16)\n",
    "    peak = np.max(np.abs(y))\n",
    "    if peak <= 0:\n",
    "        return np.zeros_like(y, dtype=np.int16)\n",
    "    y = y / peak\n",
    "    return (y * 32767.0).astype(np.int16)\n",
    "\n",
    "\n",
    "def save_augmented(\n",
    "    src_path: Path,\n",
    "    variants: List[np.ndarray],\n",
    "    out_dir: Path,\n",
    "    cfg: AugmentConfig,\n",
    ") -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stem = src_path.stem\n",
    "    suffixes = [\"ps\", \"ts\", \"noise\"]\n",
    "    for idx, y_aug in enumerate(variants):\n",
    "        tag = suffixes[idx] if idx < len(suffixes) else f\"aug{idx+1}\"\n",
    "        out_path = out_dir / f\"{stem}_{tag}.wav\"\n",
    "        wavfile.write(out_path, cfg.sr, _normalize_to_int16(y_aug))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##数据集分割代码\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths (edit for your machine)\n",
    "# ---------------------------------------------------------------------\n",
    "DATASET_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/Data/MLEndHWII_sample_800\")\n",
    "DEFAULT_FEATURES = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results/features.npz\")\n",
    "DEFAULT_OUT_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results/splits\")\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:  # pragma: no cover - fallback when tqdm is unavailable\n",
    "    tqdm = None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Config for subject-wise split\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    train_ratio: float = 0.7\n",
    "    val_ratio: float = 0.15\n",
    "    test_ratio: float = 0.15\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) IO helpers for .npz features\n",
    "# ---------------------------------------------------------------------\n",
    "def load_features_npz(path: Path) -> Dict[str, np.ndarray]:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    return {k: data[k] for k in data.files}\n",
    "\n",
    "\n",
    "def save_features_npz(\n",
    "    out_path: Path,\n",
    "    X: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    paths: np.ndarray,\n",
    "    feature_names: np.ndarray,\n",
    "    subject: np.ndarray,\n",
    "    mode: np.ndarray,\n",
    "    take: np.ndarray,\n",
    ") -> None:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        X=X,\n",
    "        labels=labels,\n",
    "        paths=paths,\n",
    "        feature_names=feature_names,\n",
    "        subject=subject,\n",
    "        mode=mode,\n",
    "        take=take,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Split by subject (group split)\n",
    "# ---------------------------------------------------------------------\n",
    "def split_by_subject(\n",
    "    subjects: np.ndarray, cfg: SplitConfig\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if not np.isclose(cfg.train_ratio + cfg.val_ratio + cfg.test_ratio, 1.0):\n",
    "        raise ValueError(\"Train/val/test ratios must sum to 1.0\")\n",
    "\n",
    "    gss = GroupShuffleSplit(\n",
    "        n_splits=1, train_size=cfg.train_ratio, random_state=cfg.seed\n",
    "    )\n",
    "    idx = np.arange(subjects.shape[0])\n",
    "    train_idx, temp_idx = next(gss.split(idx, groups=subjects))\n",
    "\n",
    "    temp_subjects = subjects[temp_idx]\n",
    "    val_ratio = cfg.val_ratio / (cfg.val_ratio + cfg.test_ratio)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, train_size=val_ratio, random_state=cfg.seed)\n",
    "    val_sub_idx, test_sub_idx = next(gss2.split(temp_idx, groups=temp_subjects))\n",
    "    val_idx = temp_idx[val_sub_idx]\n",
    "    test_idx = temp_idx[test_sub_idx]\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Augment audio and extract features\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_features_from_audio(y: np.ndarray, cfg: FeatureConfig) -> np.ndarray:\n",
    "    feats: List[float] = []\n",
    "    feats.extend(mfcc_features(y, cfg))\n",
    "    feats.extend(f0_features(y, cfg))\n",
    "    feats.extend(rhythm_features(y, cfg))\n",
    "    return np.asarray(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "def augment_and_extract(\n",
    "    paths: List[str],\n",
    "    subjects: np.ndarray,\n",
    "    modes: np.ndarray,\n",
    "    takes: np.ndarray,\n",
    "    cfg_feat: FeatureConfig,\n",
    "    cfg_aug: AugmentConfig,\n",
    "    seed: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    feats_list: List[np.ndarray] = []\n",
    "    labels_list: List[str] = []\n",
    "    paths_list: List[str] = []\n",
    "    subj_list: List[str] = []\n",
    "    mode_list: List[str] = []\n",
    "    take_list: List[str] = []\n",
    "\n",
    "    iterator = tqdm(paths, desc=\"Augmenting train\", unit=\"file\") if tqdm else paths\n",
    "    for i, p in enumerate(iterator):\n",
    "        y = load_audio(p, cfg_aug)\n",
    "        variants = augment_sample(y, cfg_aug, rng)\n",
    "        base_label = Path(p).stem.split(\"_\", 3)[-1]\n",
    "        for j, y_aug in enumerate(variants):\n",
    "            feats_list.append(extract_features_from_audio(y_aug, cfg_feat))\n",
    "            labels_list.append(base_label)\n",
    "            paths_list.append(f\"{p}::aug{j+1}\")\n",
    "            subj_list.append(str(subjects[i]))\n",
    "            mode_list.append(str(modes[i]))\n",
    "            take_list.append(str(takes[i]))\n",
    "        if tqdm is None and (i + 1) % 50 == 0:\n",
    "            print(f\"Augmented {i+1}/{len(paths)} files...\")\n",
    "\n",
    "    return (\n",
    "        np.vstack(feats_list) if feats_list else np.zeros((0, 0), dtype=np.float32),\n",
    "        np.asarray(labels_list),\n",
    "        np.asarray(paths_list),\n",
    "        np.asarray(subj_list),\n",
    "        np.asarray(mode_list),\n",
    "        np.asarray(take_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##特征可视化代码\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths (keep original .npz path)\n",
    "# ---------------------------------------------------------------------\n",
    "DEFAULT_FEATURES = Path(\n",
    "    \"/Users/panmingh/Code/ML_Coursework/MyCourse/results/splits/train_full.npz\"\n",
    ")\n",
    "DEFAULT_FIG_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results/figures\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Config\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class VizConfig:\n",
    "    max_points: int = 2000\n",
    "    random_seed: int = 42\n",
    "    tsne_perplexity: float = 30.0\n",
    "    tsne_iter: int = 1000\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Data loading\n",
    "# ---------------------------------------------------------------------\n",
    "def load_features_npz(path: Path) -> pd.DataFrame:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    features = data[\"X\"]\n",
    "    labels = data[\"labels\"].astype(str)\n",
    "    names = data[\"feature_names\"].astype(str).tolist()\n",
    "    df = pd.DataFrame(features, columns=names)\n",
    "    df[\"label\"] = labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _subset_df(df: pd.DataFrame, cfg: VizConfig) -> pd.DataFrame:\n",
    "    if len(df) <= cfg.max_points:\n",
    "        return df\n",
    "    return df.sample(n=cfg.max_points, random_state=cfg.random_seed)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Plotting helpers (show inline + optional save)\n",
    "# ---------------------------------------------------------------------\n",
    "def _finish_plot(fig: plt.Figure, out_path: Path | None, show: bool) -> None:\n",
    "    fig.tight_layout()\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_box_violin_kde(\n",
    "    df: pd.DataFrame,\n",
    "    features: Iterable[str],\n",
    "    out_dir: Path | None = None,\n",
    "    show: bool = True,\n",
    ") -> None:\n",
    "    if out_dir is not None:\n",
    "        ensure_dir(out_dir)\n",
    "    for feat in features:\n",
    "        if feat not in df.columns:\n",
    "            continue\n",
    "        values = df[feat].to_numpy()\n",
    "        if np.nanstd(values) == 0.0:\n",
    "            print(f\"[warn] skip {feat}: constant values\")\n",
    "            continue\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "        sns.boxplot(data=df, x=\"label\", y=feat, ax=axes[0])\n",
    "        axes[0].set_title(f\"Box Plot - {feat}\")\n",
    "        axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        sns.violinplot(data=df, x=\"label\", y=feat, ax=axes[1], cut=0)\n",
    "        axes[1].set_title(f\"Violin Plot - {feat}\")\n",
    "        axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        sns.kdeplot(\n",
    "            data=df,\n",
    "            x=feat,\n",
    "            hue=\"label\",\n",
    "            ax=axes[2],\n",
    "            fill=False,\n",
    "            common_norm=False,\n",
    "            warn_singular=False,\n",
    "        )\n",
    "        axes[2].set_title(f\"KDE - {feat}\")\n",
    "\n",
    "        out_path = out_dir / f\"{feat}_dist.png\" if out_dir is not None else None\n",
    "        _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def plot_corr_heatmap(\n",
    "    df: pd.DataFrame, out_dir: Path | None = None, show: bool = True\n",
    ") -> None:\n",
    "    if out_dir is not None:\n",
    "        ensure_dir(out_dir)\n",
    "    corr = df.drop(columns=[\"label\"]).corr()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0.0, ax=ax)\n",
    "    ax.set_title(\"Feature Correlation Heatmap\")\n",
    "    out_path = out_dir / \"correlation_heatmap.png\" if out_dir is not None else None\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def plot_pca_tsne(\n",
    "    df: pd.DataFrame,\n",
    "    out_dir: Path | None = None,\n",
    "    cfg: VizConfig | None = None,\n",
    "    show: bool = True,\n",
    ") -> None:\n",
    "    if out_dir is not None:\n",
    "        ensure_dir(out_dir)\n",
    "    cfg = cfg or VizConfig()\n",
    "    df_sub = _subset_df(df, cfg)\n",
    "    X = df_sub.drop(columns=[\"label\"]).to_numpy()\n",
    "    y = df_sub[\"label\"].to_numpy()\n",
    "    if X.shape[0] < 2:\n",
    "        print(\"[warn] skip PCA/t-SNE: not enough samples\")\n",
    "        return\n",
    "    std = np.nanstd(X, axis=0)\n",
    "    keep = std > 1e-8\n",
    "    if not np.any(keep):\n",
    "        print(\"[warn] skip PCA/t-SNE: all features are constant\")\n",
    "        return\n",
    "    X = X[:, keep]\n",
    "    if np.unique(X, axis=0).shape[0] < 2:\n",
    "        print(\"[warn] skip PCA/t-SNE: only one unique sample\")\n",
    "        return\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=cfg.random_seed)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, s=30, ax=ax)\n",
    "    ax.set_title(\"PCA (2D)\")\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    out_path = out_dir / \"pca_2d.png\" if out_dir is not None else None\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "    tsne_kwargs = dict(\n",
    "        n_components=2,\n",
    "        perplexity=cfg.tsne_perplexity,\n",
    "        random_state=cfg.random_seed,\n",
    "        init=\"pca\",\n",
    "        learning_rate=\"auto\",\n",
    "    )\n",
    "    try:\n",
    "        tsne = TSNE(max_iter=cfg.tsne_iter, **tsne_kwargs)\n",
    "    except TypeError:\n",
    "        tsne = TSNE(n_iter=cfg.tsne_iter, **tsne_kwargs)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, s=30, ax=ax)\n",
    "    ax.set_title(\"t-SNE (2D)\")\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    out_path = out_dir / \"tsne_2d.png\" if out_dir is not None else None\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def default_feature_list() -> List[str]:\n",
    "    return [\n",
    "        \"interval_mean\",\n",
    "        \"interval_std\",\n",
    "        \"tempo\",\n",
    "        \"ioi_std\",\n",
    "        \"f0n_std\",\n",
    "        \"mfcc_mean_1\",\n",
    "    ]\n",
    "df = load_features_npz(DEFAULT_FEATURES)\n",
    "feats = default_feature_list()\n",
    "plot_box_violin_kde(df, feats, out_dir=None, show=True)\n",
    "plot_corr_heatmap(df, out_dir=None, show=True)\n",
    "plot_pca_tsne(df, out_dir=None, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##无监督聚类评估代码\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths (keep original .npz path)\n",
    "# ---------------------------------------------------------------------\n",
    "DEFAULT_FEATURES = Path(\n",
    "    \"/Users/panmingh/Code/ML_Coursework/MyCourse/results/splits/train_full.npz\"\n",
    ")\n",
    "DEFAULT_OUT_DIR = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Config\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class UnsupervisedConfig:\n",
    "    n_clusters: int = 8\n",
    "    random_seed: int = 42\n",
    "    max_points: int = 2000\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Data helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def load_features_npz(path: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    X = data[\"X\"]\n",
    "    labels = data[\"labels\"].astype(str)\n",
    "    feature_names = data[\"feature_names\"].astype(str)\n",
    "    return X, labels, feature_names\n",
    "\n",
    "\n",
    "def subset_data(\n",
    "    X: np.ndarray, y: np.ndarray, cfg: UnsupervisedConfig\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if X.shape[0] <= cfg.max_points:\n",
    "        return X, y\n",
    "    rng = np.random.default_rng(cfg.random_seed)\n",
    "    idx = rng.choice(X.shape[0], size=cfg.max_points, replace=False)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Metrics + plots (show inline + optional save)\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_metrics(X: np.ndarray, y: np.ndarray, clusters: np.ndarray) -> Dict[str, float]:\n",
    "    nmi = normalized_mutual_info_score(y, clusters)\n",
    "    ari = adjusted_rand_score(y, clusters)\n",
    "    sil = silhouette_score(X, clusters) if len(np.unique(clusters)) > 1 else 0.0\n",
    "    return {\"nmi\": float(nmi), \"ari\": float(ari), \"silhouette\": float(sil)}\n",
    "\n",
    "\n",
    "def _finish_plot(fig: plt.Figure, out_path: Path | None, show: bool) -> None:\n",
    "    fig.tight_layout()\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_confusion(\n",
    "    y: np.ndarray,\n",
    "    clusters: np.ndarray,\n",
    "    out_path: Path | None = None,\n",
    "    show: bool = True,\n",
    ") -> None:\n",
    "    labels = np.unique(y)\n",
    "    conf = contingency_matrix(y, clusters)\n",
    "    row_sums = conf.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    conf_norm = conf / row_sums\n",
    "    df = pd.DataFrame(\n",
    "        conf_norm, index=labels, columns=[f\"C{i}\" for i in range(conf.shape[1])]\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Blues\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Cluster vs Label Confusion (Row-Normalized)\")\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Label\")\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def save_metrics(metrics: Dict[str, float], out_dir: Path | None = None) -> None:\n",
    "    if out_dir is None:\n",
    "        return\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metrics_path = out_dir / \"unsupervised_metrics.json\"\n",
    "    with metrics_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Example \"notebook flow\" (copy into cells if desired)\n",
    "# ---------------------------------------------------------------------\n",
    "cfg = UnsupervisedConfig(n_clusters=8)\n",
    "X, y, _ = load_features_npz(DEFAULT_FEATURES)\n",
    "X, y = subset_data(X, y, cfg)\n",
    "kmeans = KMeans(n_clusters=cfg.n_clusters, random_state=cfg.random_seed, n_init=10)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "metrics = compute_metrics(X, y, clusters)\n",
    "metrics\n",
    "plot_confusion(y, clusters, out_path=None, show=True)\n",
    "save_metrics(metrics, DEFAULT_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###有监督分类训练代码\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:  # pragma: no cover\n",
    "    tqdm = None\n",
    "\n",
    "import librosa\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths (keep original .npz paths)\n",
    "# ---------------------------------------------------------------------\n",
    "DEFAULT_TRAIN = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results/splits/train_full.npz\")\n",
    "DEFAULT_VAL = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results/splits/val.npz\")\n",
    "DEFAULT_OUT = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results\")\n",
    "DEFAULT_MODEL = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/models/final_model.pkl\")\n",
    "DEFAULT_MODEL_DIR = DEFAULT_MODEL.parent\n",
    "DEFAULT_MODEL_PATHS = {\n",
    "    \"knn\": DEFAULT_MODEL_DIR / \"knn_model.pkl\",\n",
    "    \"rf\": DEFAULT_MODEL_DIR / \"rf_model.pkl\",\n",
    "    \"mlp\": DEFAULT_MODEL_DIR / \"mlp_model.pkl\",\n",
    "    \"cnn\": DEFAULT_MODEL_DIR / \"cnn_model.pkl\",\n",
    "    \"final\": DEFAULT_MODEL,\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Config\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    n_splits: int = 5\n",
    "    random_seed: int = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MLPStableConfig:\n",
    "    hidden_layer_sizes: Tuple[int, int] = (128, 64)\n",
    "    alpha: float = 1e-3\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 200\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Loading + logging\n",
    "# ---------------------------------------------------------------------\n",
    "def load_features(path: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    X = data[\"X\"].astype(np.float32)\n",
    "    y = data[\"labels\"].astype(str)\n",
    "    paths = data[\"paths\"].astype(str)\n",
    "    return X, y, paths\n",
    "\n",
    "\n",
    "def log_feature_stats(name: str, X: np.ndarray) -> None:\n",
    "    finite_mask = np.isfinite(X)\n",
    "    finite_ratio = float(np.mean(finite_mask))\n",
    "    print(\n",
    "        f\"[{name}] shape={X.shape} finite={finite_ratio:.3f} \"\n",
    "        f\"min={np.nanmin(X):.4f} max={np.nanmax(X):.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def log_label_stats(name: str, y: np.ndarray) -> None:\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    dist = \", \".join([f\"{u}:{c}\" for u, c in zip(unique, counts)])\n",
    "    print(f\"[{name}] samples={len(y)} classes={len(unique)} dist={{ {dist} }}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Preprocessing helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def sanitize_features(\n",
    "    X_train: np.ndarray, X_val: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    low = np.percentile(X_train, 1, axis=0)\n",
    "    high = np.percentile(X_train, 99, axis=0)\n",
    "    X_train = np.clip(X_train, low, high)\n",
    "    X_val = np.clip(X_val, low, high)\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def compute_clip_bounds(\n",
    "    X: np.ndarray, low_q: float = 1.0, high_q: float = 99.0\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    low = np.percentile(X, low_q, axis=0)\n",
    "    high = np.percentile(X, high_q, axis=0)\n",
    "    return low, high\n",
    "\n",
    "\n",
    "def apply_clip(X: np.ndarray, low: np.ndarray, high: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(X, low, high)\n",
    "\n",
    "\n",
    "def preprocess_mlp(\n",
    "    X_train: np.ndarray, X_val: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray, StandardScaler, Tuple[np.ndarray, np.ndarray]]:\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "    X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "    low, high = compute_clip_bounds(X_train, low_q=1.0, high_q=99.0)\n",
    "    X_train = apply_clip(X_train, low, high)\n",
    "    X_val = apply_clip(X_val, low, high)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    X_train = np.clip(X_train, -3.0, 3.0)\n",
    "    X_val = np.clip(X_val, -3.0, 3.0)\n",
    "    return X_train, X_val, scaler, (low, high)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Metrics\n",
    "# ---------------------------------------------------------------------\n",
    "def macro_auc(y_true: np.ndarray, y_proba: np.ndarray, classes: List[int]) -> float:\n",
    "    try:\n",
    "        return float(\n",
    "            roc_auc_score(\n",
    "                y_true,\n",
    "                y_proba,\n",
    "                multi_class=\"ovr\",\n",
    "                average=\"macro\",\n",
    "                labels=classes,\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def evaluate_metrics(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    classes = sorted(np.unique(y_true).tolist())\n",
    "    return {\n",
    "        \"macro_auc\": macro_auc(y_true, y_proba, classes),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) CV search\n",
    "# ---------------------------------------------------------------------\n",
    "def cv_score(\n",
    "    build_model: Any,\n",
    "    param_grid: List[Dict[str, Any]],\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    cfg: TrainConfig,\n",
    ") -> Tuple[Dict[str, Any], float, int]:\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=cfg.random_seed)\n",
    "    best_params: Dict[str, Any] = {}\n",
    "    best_score = -np.inf\n",
    "    failed_folds = 0\n",
    "    iterator = tqdm(param_grid, desc=\"CV params\", unit=\"cfg\") if tqdm else param_grid\n",
    "    for params in iterator:\n",
    "        scores = []\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            model = build_model(params)\n",
    "            try:\n",
    "                model.fit(X[train_idx], y[train_idx])\n",
    "                y_proba = model.predict_proba(X[val_idx])\n",
    "                score = macro_auc(y[val_idx], y_proba, sorted(np.unique(y).tolist()))\n",
    "                if not np.isnan(score):\n",
    "                    scores.append(score)\n",
    "            except ValueError:\n",
    "                failed_folds += 1\n",
    "                continue\n",
    "        mean_score = float(np.mean(scores)) if scores else float(\"-inf\")\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "    return best_params, best_score, failed_folds\n",
    "\n",
    "\n",
    "def build_knn(params: Dict[str, Any]) -> Pipeline:\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", KNeighborsClassifier(**params)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_rf(params: Dict[str, Any]) -> RandomForestClassifier:\n",
    "    return RandomForestClassifier(random_state=42, **params)\n",
    "\n",
    "\n",
    "def train_mlp_stable(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    cfg: MLPStableConfig,\n",
    ") -> Tuple[MLPClassifier, Dict[str, float]]:\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=cfg.hidden_layer_sizes,\n",
    "        solver=\"adam\",\n",
    "        activation=\"relu\",\n",
    "        learning_rate_init=cfg.lr,\n",
    "        learning_rate=\"adaptive\",\n",
    "        alpha=cfg.alpha,\n",
    "        max_iter=cfg.epochs,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_val)\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    metrics = evaluate_metrics(y_val, y_pred, y_proba)\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) CNN utilities (optional)\n",
    "# ---------------------------------------------------------------------\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        paths: List[str],\n",
    "        labels: np.ndarray,\n",
    "        label_encoder: LabelEncoder,\n",
    "        sr: int,\n",
    "        n_mfcc: int,\n",
    "    ):\n",
    "        self.paths = paths\n",
    "        self.labels = label_encoder.transform(labels)\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        path = self.paths[idx]\n",
    "        y, _ = librosa.load(path, sr=self.sr, mono=True)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=self.n_mfcc)\n",
    "        return mfcc.astype(np.float32), int(self.labels[idx])\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_mfcc: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(n_mfcc, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.net(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def collate_pad(batch: List[Tuple[np.ndarray, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    lengths = [b[0].shape[1] for b in batch]\n",
    "    max_len = max(lengths)\n",
    "    n_mfcc = batch[0][0].shape[0]\n",
    "    padded = np.zeros((len(batch), n_mfcc, max_len), dtype=np.float32)\n",
    "    labels = np.zeros(len(batch), dtype=np.int64)\n",
    "    for i, (mfcc, label) in enumerate(batch):\n",
    "        padded[i, :, : mfcc.shape[1]] = mfcc\n",
    "        labels[i] = label\n",
    "    return torch.from_numpy(padded), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "def train_cnn(\n",
    "    train_paths: List[str],\n",
    "    train_labels: np.ndarray,\n",
    "    val_paths: List[str],\n",
    "    val_labels: np.ndarray,\n",
    "    label_encoder: LabelEncoder,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 16,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[Dict[str, float], Dict[str, Any]]:\n",
    "    if torch is None:\n",
    "        raise RuntimeError(\"PyTorch is required for CNN training.\")\n",
    "\n",
    "    n_mfcc = 13\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "    train_ds = MFCCDataset(train_paths, train_labels, label_encoder, sr=22050, n_mfcc=n_mfcc)\n",
    "    val_ds = MFCCDataset(val_paths, val_labels, label_encoder, sr=22050, n_mfcc=n_mfcc)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_pad)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n",
    "\n",
    "    model = SimpleCNN(n_mfcc=n_mfcc, n_classes=n_classes).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        batch_iter = train_loader\n",
    "        if tqdm:\n",
    "            batch_iter = tqdm(\n",
    "                train_loader,\n",
    "                desc=f\"CNN train {epoch}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                leave=False,\n",
    "            )\n",
    "        for xb, yb in batch_iter:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optim.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_true.append(yb.numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_proba = np.vstack(all_probs)\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    metrics = evaluate_metrics(y_true, y_pred, y_proba)\n",
    "    return metrics, {\"model\": model, \"label_encoder\": label_encoder}\n",
    "\n",
    "\n",
    "def filter_existing_audio(paths: np.ndarray, labels: np.ndarray) -> Tuple[List[str], np.ndarray]:\n",
    "    out_paths: List[str] = []\n",
    "    out_labels: List[str] = []\n",
    "    for p, y in zip(paths, labels):\n",
    "        if \"::\" in p:\n",
    "            continue\n",
    "        if Path(p).is_file():\n",
    "            out_paths.append(p)\n",
    "            out_labels.append(y)\n",
    "    return out_paths, np.asarray(out_labels)\n",
    "\n",
    "\n",
    "def save_payload(payload: Dict[str, Any], path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    dump(payload, path)\n",
    "\n",
    "\n",
    "def retrain_and_save_sklearn_models(\n",
    "    results: Dict[str, Any],\n",
    "    X_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_train_enc: np.ndarray,\n",
    "    y_val_enc: np.ndarray,\n",
    "    label_encoder: LabelEncoder,\n",
    "    mlp_cfg: MLPStableConfig,\n",
    "    model_paths: Dict[str, Path] | None = None,\n",
    ") -> Dict[str, Path]:\n",
    "    paths = model_paths or DEFAULT_MODEL_PATHS\n",
    "    saved: Dict[str, Path] = {}\n",
    "    X_full = np.vstack([X_train, X_val])\n",
    "    y_full = np.concatenate([y_train_enc, y_val_enc])\n",
    "\n",
    "    if \"knn\" in results and \"best_params\" in results[\"knn\"]:\n",
    "        model = build_knn(results[\"knn\"][\"best_params\"])\n",
    "        model.fit(X_full, y_full)\n",
    "        save_payload({\"model\": model, \"label_encoder\": label_encoder}, paths[\"knn\"])\n",
    "        saved[\"knn\"] = paths[\"knn\"]\n",
    "\n",
    "    if \"rf\" in results and \"best_params\" in results[\"rf\"]:\n",
    "        model = build_rf(results[\"rf\"][\"best_params\"])\n",
    "        model.fit(X_full, y_full)\n",
    "        save_payload({\"model\": model, \"label_encoder\": label_encoder}, paths[\"rf\"])\n",
    "        saved[\"rf\"] = paths[\"rf\"]\n",
    "\n",
    "    if \"mlp\" in results and \"config\" in results[\"mlp\"]:\n",
    "        X_full_mlp, _, mlp_scaler, clip_bounds = preprocess_mlp(X_full, X_full)\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=mlp_cfg.hidden_layer_sizes,\n",
    "            solver=\"adam\",\n",
    "            activation=\"relu\",\n",
    "            learning_rate_init=mlp_cfg.lr,\n",
    "            learning_rate=\"adaptive\",\n",
    "            alpha=mlp_cfg.alpha,\n",
    "            max_iter=mlp_cfg.epochs,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(X_full_mlp, y_full)\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"label_encoder\": label_encoder,\n",
    "            \"scaler\": mlp_scaler,\n",
    "            \"clip_bounds\": clip_bounds,\n",
    "        }\n",
    "        save_payload(payload, paths[\"mlp\"])\n",
    "        saved[\"mlp\"] = paths[\"mlp\"]\n",
    "\n",
    "    return saved\n",
    "\n",
    "\n",
    "def save_cnn_artifacts(\n",
    "    cnn_artifacts: Dict[str, Any] | None, model_paths: Dict[str, Path] | None = None\n",
    ") -> Path | None:\n",
    "    if cnn_artifacts is None:\n",
    "        return None\n",
    "    paths = model_paths or DEFAULT_MODEL_PATHS\n",
    "    save_payload(cnn_artifacts, paths[\"cnn\"])\n",
    "    return paths[\"cnn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] shape=(2252, 78) finite=1.000 min=-741.9331 max=258.3984\n",
      "[val] shape=(115, 78) finite=1.000 min=-715.3228 max=258.3984\n",
      "[train] samples=2252 classes=8 dist={ Feeling:276, Friend:292, Happy:284, Married:288, Necessities:284, NewYork:272, RememberMe:284, TryEverything:272 }\n",
      "[val] samples=115 classes=8 dist={ Feeling:14, Friend:15, Happy:14, Married:12, Necessities:14, NewYork:16, RememberMe:15, TryEverything:15 }\n",
      "[paths] train_paths=2252 val_paths=115\n",
      "[knn] cv_folds=5 grid_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV params: 100%|██████████| 8/8 [00:00<00:00, 36.92cfg/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[knn] cv_macro_auc=0.9430 val_auc=0.5652 val_acc=0.2000 val_f1=0.1948 failed_folds=0\n",
      "[rf] cv_folds=5 grid_size=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV params: 100%|██████████| 6/6 [00:19<00:00,  3.21s/cfg]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rf] cv_macro_auc=0.9775 val_auc=0.7795 val_acc=0.4000 val_f1=0.3937 failed_folds=0\n",
      "[mlp] val_auc=0.7301 val_acc=0.3217 val_f1=0.3163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved sklearn models: {'knn': PosixPath('/Users/panmingh/Code/ML_Coursework/MyCourse/models/knn_model.pkl'), 'rf': PosixPath('/Users/panmingh/Code/ML_Coursework/MyCourse/models/rf_model.pkl'), 'mlp': PosixPath('/Users/panmingh/Code/ML_Coursework/MyCourse/models/mlp_model.pkl')}\n",
      "saved cnn model: /Users/panmingh/Code/ML_Coursework/MyCourse/models/cnn_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train执行代码\n",
    "# ---------------------------------------------------------------------\n",
    "# 8) Example \"notebook flow\" (copy into cells if desired)\n",
    "# ---------------------------------------------------------------------\n",
    "X_train, y_train, train_paths = load_features(DEFAULT_TRAIN)\n",
    "X_val, y_val, val_paths = load_features(DEFAULT_VAL)\n",
    "X_train, X_val = sanitize_features(X_train, X_val)\n",
    "log_feature_stats(\"train\", X_train)\n",
    "log_feature_stats(\"val\", X_val)\n",
    "log_label_stats(\"train\", y_train)\n",
    "log_label_stats(\"val\", y_val)\n",
    "print(f\"[paths] train_paths={len(train_paths)} val_paths={len(val_paths)}\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(np.concatenate([y_train, y_val]))\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_val_enc = le.transform(y_val)\n",
    "\n",
    "cfg = TrainConfig()\n",
    "results: Dict[str, Any] = {}\n",
    "\n",
    "knn_grid = [\n",
    "    {\"n_neighbors\": k, \"weights\": w}\n",
    "    for k in [3, 5, 7, 9]\n",
    "    for w in [\"uniform\", \"distance\"]\n",
    "]\n",
    "rf_grid = [\n",
    "    {\"n_estimators\": n, \"max_depth\": d}\n",
    "    for n in [100, 200]\n",
    "    for d in [5, 10, None]\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "for name, builder, grid in [\n",
    "    (\"knn\", build_knn, knn_grid),\n",
    "    (\"rf\", build_rf, rf_grid),\n",
    "]:\n",
    "    if tqdm is None:\n",
    "        print(f\"Training {name}...\")\n",
    "    print(f\"[{name}] cv_folds={cfg.n_splits} grid_size={len(grid)}\")\n",
    "    best_params, cv_auc, failed_folds = cv_score(builder, grid, X_train, y_train_enc, cfg)\n",
    "    try:\n",
    "        model = builder(best_params)\n",
    "        model.fit(X_train, y_train_enc)\n",
    "        y_proba = model.predict_proba(X_val)\n",
    "        y_pred = np.argmax(y_proba, axis=1)\n",
    "        metrics = evaluate_metrics(y_val_enc, y_pred, y_proba)\n",
    "        results[name] = {\"best_params\": best_params, \"cv_macro_auc\": cv_auc, \"val\": metrics}\n",
    "        print(\n",
    "            f\"[{name}] cv_macro_auc={cv_auc:.4f} \"\n",
    "            f\"val_auc={metrics['macro_auc']:.4f} \"\n",
    "            f\"val_acc={metrics['accuracy']:.4f} \"\n",
    "            f\"val_f1={metrics['macro_f1']:.4f} \"\n",
    "            f\"failed_folds={failed_folds}\"\n",
    "        )\n",
    "    except ValueError as exc:\n",
    "        results[name] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"cv_macro_auc\": cv_auc,\n",
    "            \"error\": str(exc),\n",
    "        }\n",
    "        print(f\"[{name}] failed after CV: {exc} failed_folds={failed_folds}\")\n",
    "\n",
    "mlp_cfg = MLPStableConfig()\n",
    "X_train_mlp, X_val_mlp, mlp_scaler, clip_bounds = preprocess_mlp(X_train, X_val)\n",
    "try:\n",
    "    mlp_model, mlp_metrics = train_mlp_stable(\n",
    "        X_train_mlp, y_train_enc, X_val_mlp, y_val_enc, mlp_cfg\n",
    "    )\n",
    "    results[\"mlp\"] = {\n",
    "        \"config\": mlp_cfg.__dict__,\n",
    "        \"val\": mlp_metrics,\n",
    "    }\n",
    "    print(\n",
    "        f\"[mlp] val_auc={mlp_metrics['macro_auc']:.4f} \"\n",
    "        f\"val_acc={mlp_metrics['accuracy']:.4f} \"\n",
    "        f\"val_f1={mlp_metrics['macro_f1']:.4f}\"\n",
    "    )\n",
    "except ValueError as exc:\n",
    "    results[\"mlp\"] = {\"config\": mlp_cfg.__dict__, \"error\": str(exc)}\n",
    "    print(f\"[mlp] failed: {exc}\")\n",
    "\n",
    "# Optional CNN (requires torch)\n",
    "include_cnn = True\n",
    "if include_cnn and torch is not None:\n",
    "    train_audio, y_train_audio = filter_existing_audio(train_paths, y_train)\n",
    "    val_audio, y_val_audio = filter_existing_audio(val_paths, y_val)\n",
    "    if train_audio and val_audio:\n",
    "        cnn_metrics, cnn_artifacts = train_cnn(\n",
    "            train_audio,\n",
    "            y_train_audio,\n",
    "            val_audio,\n",
    "            y_val_audio,\n",
    "            le,\n",
    "            epochs=20,\n",
    "        )\n",
    "        results[\"cnn\"] = {\"val\": cnn_metrics}\n",
    "    else:\n",
    "        results[\"cnn\"] = {\"error\": \"No valid audio paths for CNN.\"}\n",
    "\n",
    "def _score(name: str) -> float:\n",
    "    val_score = results.get(name, {}).get(\"val\", {}).get(\"macro_auc\", float(\"-inf\"))\n",
    "    return val_score if not np.isnan(val_score) else float(\"-inf\")\n",
    "\n",
    "best_name = max(results.keys(), key=_score)\n",
    "results[\"best_model\"] = best_name\n",
    "\n",
    "DEFAULT_OUT.mkdir(parents=True, exist_ok=True)\n",
    "with (DEFAULT_OUT / \"train_metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if best_name == \"cnn\":\n",
    "    payload = cnn_artifacts\n",
    "else:\n",
    "    # Retrain best sklearn model on Train+Val\n",
    "    X_full = np.vstack([X_train, X_val])\n",
    "    y_full = np.concatenate([y_train_enc, y_val_enc])\n",
    "    if best_name == \"knn\":\n",
    "        final_model = build_knn(results[\"knn\"][\"best_params\"])\n",
    "        final_model.fit(X_full, y_full)\n",
    "        payload = {\"model\": final_model, \"label_encoder\": le}\n",
    "    elif best_name == \"rf\":\n",
    "        final_model = build_rf(results[\"rf\"][\"best_params\"])\n",
    "        final_model.fit(X_full, y_full)\n",
    "        payload = {\"model\": final_model, \"label_encoder\": le}\n",
    "    else:\n",
    "        X_full_mlp, _, mlp_scaler, clip_bounds = preprocess_mlp(X_full, X_full)\n",
    "        final_model = MLPClassifier(\n",
    "            hidden_layer_sizes=mlp_cfg.hidden_layer_sizes,\n",
    "            solver=\"adam\",\n",
    "            activation=\"relu\",\n",
    "            learning_rate_init=mlp_cfg.lr,\n",
    "            learning_rate=\"adaptive\",\n",
    "            alpha=mlp_cfg.alpha,\n",
    "            max_iter=mlp_cfg.epochs,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=42,\n",
    "        )\n",
    "        final_model.fit(X_full_mlp, y_full)\n",
    "        payload = {\n",
    "            \"model\": final_model,\n",
    "            \"label_encoder\": le,\n",
    "            \"scaler\": mlp_scaler,\n",
    "            \"clip_bounds\": clip_bounds,\n",
    "        }\n",
    "    DEFAULT_MODEL.parent.mkdir(parents=True, exist_ok=True)\n",
    "    dump(payload, DEFAULT_MODEL)\n",
    "\n",
    "# Save all sklearn models (KNN/RF/MLP) for multi-model evaluation\n",
    "saved_paths = retrain_and_save_sklearn_models(\n",
    "    results,\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train_enc,\n",
    "    y_val_enc,\n",
    "    le,\n",
    "    mlp_cfg,\n",
    "    model_paths=DEFAULT_MODEL_PATHS,\n",
    ")\n",
    "print(f\"saved sklearn models: {saved_paths}\")\n",
    "\n",
    "# Save CNN if trained\n",
    "if \"cnn\" in results and \"val\" in results[\"cnn\"]:\n",
    "    cnn_path = save_cnn_artifacts(cnn_artifacts, model_paths=DEFAULT_MODEL_PATHS)\n",
    "    print(f\"saved cnn model: {cnn_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eval代码\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from joblib import load\n",
    "except Exception:  # pragma: no cover\n",
    "    load = None\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "except Exception:  # pragma: no cover\n",
    "    torch = None\n",
    "    nn = None\n",
    "    DataLoader = None\n",
    "    Dataset = object\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "except Exception:  # pragma: no cover\n",
    "    librosa = None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Paths (keep original .npz/model paths)\n",
    "# ---------------------------------------------------------------------\n",
    "DEFAULT_TEST = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results/splits/test.npz\")\n",
    "DEFAULT_MODEL = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/models/final_model.pkl\")\n",
    "DEFAULT_OUT = Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/results\")\n",
    "DEFAULT_MODELS = {\"final\": DEFAULT_MODEL}\n",
    "METRIC_ORDER = (\"accuracy\", \"macro_f1\", \"macro_auc\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Config\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    out_dir: Path = DEFAULT_OUT\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Loading + preprocessing\n",
    "# ---------------------------------------------------------------------\n",
    "def load_features(path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    X = data[\"X\"].astype(np.float32)\n",
    "    y = data[\"labels\"].astype(str)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_features_with_paths(path: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    X = data[\"X\"].astype(np.float32)\n",
    "    y = data[\"labels\"].astype(str)\n",
    "    paths = data[\"paths\"].astype(str)\n",
    "    return X, y, paths\n",
    "\n",
    "\n",
    "def sanitize_features(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    low = np.percentile(X, 1, axis=0)\n",
    "    high = np.percentile(X, 99, axis=0)\n",
    "    return np.clip(X, low, high)\n",
    "\n",
    "\n",
    "def load_model(path: Path):\n",
    "    if load is None:\n",
    "        raise RuntimeError(\"joblib is required to load the model.\")\n",
    "    payload = load(path)\n",
    "    if isinstance(payload, dict) and \"model\" in payload:\n",
    "        return payload[\"model\"], payload.get(\"label_encoder\")\n",
    "    return payload, None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Metrics + plots (show inline + optional save)\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_metrics(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    labels = sorted(np.unique(y_true).tolist())\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\")),\n",
    "    }\n",
    "    try:\n",
    "        metrics[\"macro_auc\"] = float(\n",
    "            roc_auc_score(\n",
    "                y_true,\n",
    "                y_proba,\n",
    "                multi_class=\"ovr\",\n",
    "                average=\"macro\",\n",
    "                labels=labels,\n",
    "            )\n",
    "        )\n",
    "    except Exception:\n",
    "        metrics[\"macro_auc\"] = float(\"nan\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _finish_plot(fig: plt.Figure, out_path: Path | None, show: bool) -> None:\n",
    "    fig.tight_layout()\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    out_path: Path | None = None,\n",
    "    show: bool = True,\n",
    "    title_suffix: str | None = None,\n",
    ") -> None:\n",
    "    labels = sorted(np.unique(y_true).tolist())\n",
    "    conf = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf, display_labels=labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
    "    title = \"Confusion Matrix (Test)\"\n",
    "    if title_suffix:\n",
    "        title = f\"{title} - {title_suffix}\"\n",
    "    ax.set_title(title)\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def plot_confusion_normalized(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    out_path: Path | None = None,\n",
    "    show: bool = True,\n",
    "    title_suffix: str | None = None,\n",
    ") -> None:\n",
    "    labels = sorted(np.unique(y_true).tolist())\n",
    "    conf = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    row_sums = conf.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    conf_norm = conf / row_sums\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        conf_norm,\n",
    "        cmap=\"Blues\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=ax,\n",
    "    )\n",
    "    title = \"Confusion Matrix (Row-Normalized)\"\n",
    "    if title_suffix:\n",
    "        title = f\"{title} - {title_suffix}\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def plot_roc_curves(\n",
    "    y_true: np.ndarray,\n",
    "    y_proba: np.ndarray,\n",
    "    out_path: Path | None = None,\n",
    "    show: bool = True,\n",
    "    title_suffix: str | None = None,\n",
    ") -> None:\n",
    "    labels = sorted(np.unique(y_true).tolist())\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for i, label in enumerate(labels):\n",
    "        y_bin = (y_true == label).astype(int)\n",
    "        fpr, tpr, _ = roc_curve(y_bin, y_proba[:, i])\n",
    "        ax.plot(fpr, tpr, label=f\"{label} (AUC {auc(fpr, tpr):.2f})\")\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "    title = \"ROC Curves (One-vs-Rest)\"\n",
    "    if title_suffix:\n",
    "        title = f\"{title} - {title_suffix}\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend(fontsize=8, loc=\"lower right\")\n",
    "    _finish_plot(fig, out_path, show)\n",
    "\n",
    "\n",
    "def save_metrics(metrics: Dict[str, float], out_dir: Path | None = None) -> None:\n",
    "    if out_dir is None:\n",
    "        return\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with (out_dir / \"test_metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def save_metrics_report(\n",
    "    metrics_by_model: Dict[str, Dict[str, float]], out_dir: Path | None = None\n",
    ") -> None:\n",
    "    if out_dir is None:\n",
    "        return\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with (out_dir / \"test_metrics_all.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics_by_model, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def print_metrics_table(metrics_by_model: Dict[str, Dict[str, float]]) -> None:\n",
    "    if not metrics_by_model:\n",
    "        return\n",
    "    keys = [k for k in METRIC_ORDER if any(k in m for m in metrics_by_model.values())]\n",
    "    header = [\"model\", *keys]\n",
    "    rows = []\n",
    "    for model_name, metrics in metrics_by_model.items():\n",
    "        row = [model_name]\n",
    "        for key in keys:\n",
    "            value = metrics.get(key, float(\"nan\"))\n",
    "            if np.isfinite(value):\n",
    "                row.append(f\"{value:.4f}\")\n",
    "            else:\n",
    "                row.append(\"nan\")\n",
    "        rows.append(row)\n",
    "    widths = [max(len(row[i]) for row in [header, *rows]) for i in range(len(header))]\n",
    "    line = \" | \".join(cell.ljust(widths[i]) for i, cell in enumerate(header))\n",
    "    sep = \"-+-\".join(\"-\" * w for w in widths)\n",
    "    print(line)\n",
    "    print(sep)\n",
    "    for row in rows:\n",
    "        print(\" | \".join(cell.ljust(widths[i]) for i, cell in enumerate(row)))\n",
    "\n",
    "\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, paths: List[str], labels: np.ndarray, sr: int, n_mfcc: int):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        path = self.paths[idx]\n",
    "        y, _ = librosa.load(path, sr=self.sr, mono=True)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=self.n_mfcc)\n",
    "        return mfcc.astype(np.float32), int(self.labels[idx])\n",
    "\n",
    "\n",
    "def collate_pad(batch: List[Tuple[np.ndarray, int]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    lengths = [b[0].shape[1] for b in batch]\n",
    "    max_len = max(lengths)\n",
    "    n_mfcc = batch[0][0].shape[0]\n",
    "    padded = np.zeros((len(batch), n_mfcc, max_len), dtype=np.float32)\n",
    "    labels = np.zeros(len(batch), dtype=np.int64)\n",
    "    for i, (mfcc, label) in enumerate(batch):\n",
    "        padded[i, :, : mfcc.shape[1]] = mfcc\n",
    "        labels[i] = label\n",
    "    return torch.from_numpy(padded), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "def _filter_existing_audio(paths: np.ndarray, labels: np.ndarray) -> Tuple[List[str], np.ndarray]:\n",
    "    out_paths: List[str] = []\n",
    "    out_labels: List[str] = []\n",
    "    for p, y in zip(paths, labels):\n",
    "        if \"::\" in p:\n",
    "            continue\n",
    "        if Path(p).is_file():\n",
    "            out_paths.append(p)\n",
    "            out_labels.append(y)\n",
    "    return out_paths, np.asarray(out_labels)\n",
    "\n",
    "\n",
    "def is_torch_model(model: object) -> bool:\n",
    "    if torch is None or nn is None:\n",
    "        return False\n",
    "    return isinstance(model, nn.Module)\n",
    "\n",
    "\n",
    "def predict_proba_cnn(\n",
    "    model: nn.Module,\n",
    "    paths: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    batch_size: int = 16,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if torch is None or librosa is None:\n",
    "        raise RuntimeError(\"PyTorch and librosa are required for CNN evaluation.\")\n",
    "    audio_paths, labels = _filter_existing_audio(paths, y_true)\n",
    "    if len(audio_paths) == 0:\n",
    "        raise RuntimeError(\"No valid audio paths found for CNN evaluation.\")\n",
    "    dataset = MFCCDataset(audio_paths, labels, sr=22050, n_mfcc=13)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_true.append(yb.numpy())\n",
    "    return np.concatenate(all_true), np.vstack(all_probs)\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    model_paths: Dict[str, Path],\n",
    "    test_path: Path = DEFAULT_TEST,\n",
    "    out_dir: Path | None = DEFAULT_OUT,\n",
    "    show: bool = True,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    X_test, y_test, test_paths = load_features_with_paths(test_path)\n",
    "    X_test = sanitize_features(X_test)\n",
    "    metrics_by_model: Dict[str, Dict[str, float]] = {}\n",
    "    for model_name, model_path in model_paths.items():\n",
    "        if not model_path.exists():\n",
    "            print(f\"[warn] skip {model_name}: missing {model_path}\")\n",
    "            continue\n",
    "        model, label_encoder = load_model(model_path)\n",
    "        y_true = label_encoder.transform(y_test) if label_encoder is not None else y_test\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            y_pred = np.argmax(y_proba, axis=1)\n",
    "        elif is_torch_model(model):\n",
    "            y_true, y_proba = predict_proba_cnn(model, test_paths, y_true)\n",
    "            y_pred = np.argmax(y_proba, axis=1)\n",
    "        else:\n",
    "            print(f\"[warn] skip {model_name}: unsupported model type\")\n",
    "            continue\n",
    "        metrics = compute_metrics(y_true, y_pred, y_proba)\n",
    "        metrics_by_model[model_name] = metrics\n",
    "        fig_dir = out_dir / \"figures\" / model_name if out_dir is not None else None\n",
    "        if fig_dir is not None:\n",
    "            fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plot_confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            out_path=(fig_dir / \"test_confusion.png\") if fig_dir is not None else None,\n",
    "            show=show,\n",
    "            title_suffix=model_name,\n",
    "        )\n",
    "        plot_confusion_normalized(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            out_path=(fig_dir / \"test_confusion_norm.png\") if fig_dir is not None else None,\n",
    "            show=show,\n",
    "            title_suffix=model_name,\n",
    "        )\n",
    "        plot_roc_curves(\n",
    "            y_true,\n",
    "            y_proba,\n",
    "            out_path=(fig_dir / \"test_roc_curves.png\") if fig_dir is not None else None,\n",
    "            show=show,\n",
    "            title_suffix=model_name,\n",
    "        )\n",
    "    print_metrics_table(metrics_by_model)\n",
    "    if out_dir is not None:\n",
    "        save_metrics_report(metrics_by_model, out_dir)\n",
    "    return metrics_by_model\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Example \"notebook flow\" (copy into cells if desired)\n",
    "# ---------------------------------------------------------------------\n",
    "models = {\n",
    "    \"knn\": Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/models/knn_model.pkl\"),\n",
    "    \"rf\": Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/models/rf_model.pkl\"),\n",
    "    \"mlp\": Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/models/mlp_model.pkl\"),\n",
    "    \"cnn\": Path(\"/Users/panmingh/Code/ML_Coursework/MyCourse/models/cnn_model.pkl\"),\n",
    "}\n",
    "evaluate_models(models, test_path=DEFAULT_TEST, out_dir=DEFAULT_OUT, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panmingh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
